# 人工智慧第一週筆記
## 課程所使用程式
* 使用語言及函式庫 例如:python、pytorch、numpy
## 傳統用法
* 搜尋+優化
## 機器學習
* 統計
* 神經網路
    * 深度學習(使用PyTorch)
## 應用方面
* 影像
    * 特徵
    * CNN(YOLO)
* 語言
    * 規則
    * 統計
    * RNN
* 下棋
    * MCTS
* 自動控制
    * 無人機/車
## 優化
* 數學
    * 科學計算(numpy,sc.py)
    * 機率統計
    * 微積分
        * 偏微分建立在梯度上面
    * 代數(線性代數、泛函)
    * 幾何(微分幾何)
## AI落地
* 在工業應用比傳統更具有優勢且具有商業價值

## 爬山演算法
* 爬山演算法 (Hill Climbing) 是一種最簡單的優化算法，該方法就像模擬人類爬山時的行為而設計的，因此稱為爬山演算法。
* 一直往低的地方走，一直走到最低點，然後你會看到左右兩邊都沒辦法更低了，於是就停止尋找，傳回該最低點作為答案。這個方法，就像是水往低處流一樣，不斷的往更低的方向流，最後一定會流到一個山谷，然後就積成一個湖了。
* 如果我們想要找的是最高點，而不是最低點，那整個行為就會像爬山一樣，只是最後爬到山頂就會停了。
# 簡易爬山演算法 -- 針對單變數函數
* hillClimbing1.py
    * 負的比較少就是高點
```
# 簡易爬山演算法
def hillClimbing(f, x, dx=0.01):
    while (True):
        print('x={0:.3f} f(x)={1:.3f}'.format(x, f(x)))
        if f(x+dx)>f(x): # 如果右邊的高度 f(x+dx) > 目前高度 f(x) ，那麼就往右走
            x = x + dx
        elif f(x-dx)>f(x): # 如果左邊的高度 f(x-dx) > 目前高度 f(x) ，那麼就往左走
            x = x - dx
        else: # 如果兩邊都沒有比現在的 f(x) 高，那麼這裡就是區域最高點，直接中斷傳回
            break
    return x

# 高度函數
def f(x):
    return -1*(x*x-2*x+1)
    # return -1*(x*x+3*x+5)
    # return -1*abs(x*x-4)

hillClimbing(f, 0) # 以 x=0 為起點，開始呼叫爬山演算法
```
* 執行結果
```
PS C:\Users\柯泓吉\Desktop\課程\人工智慧\ai\02-optimize\01-hillclimbing\02-var1> python hillClimbing1.py
x=0.000 f(x)=-1.000
x=0.010 f(x)=-0.980
x=0.020 f(x)=-0.960
.
.
.
x=0.990 f(x)=-0.000
x=1.000 f(x)=-0.000
```
* hillClimbing2.py
```
import random

def hillClimbing(f, x, y, h=0.01):
    while (True):
        fxy = f(x, y)
        print('x={0:.3f} y={1:.3f} f(x,y)={2:.3f}'.format(x, y, fxy))
        if f(x+h, y) >= fxy:
            x = x + h
        elif f(x-h, y) >= fxy:
            x = x - h
        elif f(x, y+h) >= fxy:
            y = y + h
        elif f(x, y-h) >= fxy:
            y = y - h
        else:
            break
    return (x,y,fxy)

def f(x, y):
    return -1 * ( x*x - 2*x + y*y + 2*y - 8 )

hillClimbing(f, 0, 0)
```
* 執行結果
```
PS C:\Users\柯泓吉\Desktop\課程\人工智慧\ai\02-optimize\01-hillclimbing\03-var2> python hillClimbing2.py
x=0.000 y=0.000 f(x,y)=8.000
x=0.010 y=0.000 f(x,y)=8.020
x=0.020 y=0.000 f(x,y)=8.040
.
.
.
x=1.000 y=-0.980 f(x,y)=10.000
x=1.000 y=-0.990 f(x,y)=10.000
x=1.000 y=-1.000 f(x,y)=10.000
```
* hillClimbing2r.py
```
import random

def hillClimbing(f, x, y, h=0.01):
    failCount = 0
    while (failCount < 10000):
        fxy = f(x, y)
        dx = random.uniform(-h, h)
        dy = random.uniform(-h, h)
        if f(x+dx, y+dy) >= fxy:
            x = x + dx
            y = y + dy
            print('x={0:.3f} y={1:.3f} f(x,y)={2:.3f}'.format(x, y, fxy))
        else:
            failCount = failCount + 1
    return (x,y,fxy)

def f(x, y):
    return -1 * ( x*x -2*x + y*y +2*y - 8 )

hillClimbing(f, 0, 0)

```
* 執行結果
```
PS C:\Users\柯泓吉\Desktop\課程\人工智慧\ai\02-optimize\01-hillclimbing\03-var2> python hillClimbing2r.py
x=0.004 y=0.002 f(x,y)=8.000
x=0.013 y=0.007 f(x,y)=8.005 
x=0.023 y=-0.001 f(x,y)=8.013
.
.
.
x=0.997 y=-1.000 f(x,y)=10.000
x=0.997 y=-0.998 f(x,y)=10.000
x=1.002 y=-0.998 f(x,y)=10.000
x=0.999 y=-0.998 f(x,y)=10.000
x=1.000 y=-1.000 f(x,y)=10.000
x=1.000 y=-1.000 f(x,y)=10.000
```