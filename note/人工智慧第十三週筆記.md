# 人工智慧第十三週筆記
## Hacker's guide to Neural Networks
* [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)
* 反傳遞演算法
    * 反向傳播（英語：Backpropagation，縮寫為BP）是「誤差反向傳播」的簡稱，是一種與最優化方法（如梯度下降法）結合使用的，用來訓練人工神經網絡的常見方法。該方法對網絡中所有權重計算損失函數的梯度。這個梯度會反饋給最優化方法，用來更新權值以最小化損失函數。
    * 反傳遞的原理主要來自偏微分的鏈鎖規則
    * [反傳遞演算法](http://programmermedia.org/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/07-neural/03-net/%E5%8F%8D%E5%82%B3%E9%81%9E%E6%BC%94%E7%AE%97%E6%B3%95%E6%89%8B%E7%AE%97%E6%A1%88%E4%BE%8B.md)
![pic1]()
![pic2]()
![pic3]()
![pic4]()
![pic5]()
![pic6]()
![pic7]()
## pytorch
* 自動求微分
* pip install torch 
    * 當出現強制結束 or killed時，就先第一檢查一下記憶體(大概率記憶體不足造成的)
* x.norm: x函數中的變數值相加平方開根號。
* torch.tensor: 單一數據類型元素的多维矩陣。
* x.grad: 該節點的梯度。
* f.backward: 求出函式f的反傳遞。
* x.item(): 從張量x中找出元素值
## code
* net1.py
```
from net import Net
net = Net()

x = net.variable(1)
y = net.variable(3)
x2 = net.mul(x, x)
y2 = net.mul(y, y)
o  = net.add(x2, y2)

print('net.forward()=', net.forward())
print('net.backwward()')
net.backward()
print('x=', x, 'y=', y, 'o=', o)
print('gfx = x.g/o.g = ', x.g/o.g, 'gfy = y.g/o.g=', y.g/o.g)
```
* 執行結果
```
ko@ko-VirtualBox:~/ai/07-neural/03-net$ python3 net1.py
net.forward()= 10
net.backwward()
x= v:1 g:2 y= v:3 g:6 o= v:10 g:1
gfx = x.g/o.g =  2.0 gfy = y.g/o.g= 6.0
```
* net2.py
```
from net import Net
net = Net()

x = net.variable(1)
y = net.variable(3)
x2 = net.mul(x, x)
y2 = net.mul(y, y)
o  = net.add(x2, y2)

net.gradient_descendent()
print('x=', x.v, 'y=', y.v)
```
* 執行結果
```
ko@ko-VirtualBox:~/ai/07-neural/03-net$ python3 net2.py
0  =>  10
1  =>  9.216
2  =>  8.4934656
3  =>  7.827577896960003
4  =>  7.213895789838339
5  =>  6.648326359915014
6  =>  6.127097573297678
7  =>  5.646733123551139
8  =>  5.2040292466647315
9  =>  4.796033353726214
10  =>  4.42002433879408
11  =>  4.073494430632624
12  =>  3.754132467271026
13  =>  3.4598084818369785
14  =>  3.1885594968609583
15  =>  2.93857643230706
16  =>  2.708192040014186
17  =>  2.4958697840770734
18  =>  2.3001935930054302
19  =>  2.1198584153138045
20  =>  1.9536615155532018
21  =>  1.800494452733831
22  =>  1.659335687639499
23  =>  1.5292437697285624
24  =>  1.4093510581818431
25  =>  1.2988579352203862
26  =>  1.1970274730991084
27  =>  1.103180519208138
28  =>  1.01669116650222
29  =>  0.936982579048446
30  =>  0.8635231448510479
31  =>  0.795822930294726
32  =>  0.7334304125596195
33  =>  0.6759294682149453
34  =>  0.6229365979068935
35  =>  0.5740983686309932
36  =>  0.5290890565303232
37  =>  0.48760847449834593
38  =>  0.44937997009767566
39  =>  0.4141485804420178
40  =>  0.38167933173536356
41  =>  0.35175567212731096
42  =>  0.3241780274325298
43  =>  0.2987624700818194
44  =>  0.27533949242740474
45  =>  0.25375287622109627
46  =>  0.23385865072536233
47  =>  0.21552413250849395
48  =>  0.19862704051982802
49  =>  0.1830546805430735
50  =>  0.1687031935884965
51  =>  0.15547686321115836
52  =>  0.14328747713540352
53  =>  0.1320537389279879
54  =>  0.12170072579603364
55  =>  0.1121593888936246
56  =>  0.10336609280436443
57  =>  0.09526219112850225
58  =>  0.08779363534402765
59  =>  0.08091061433305591
60  =>  0.07456722216934435
61  =>  0.06872115195126777
62  =>  0.06333341363828837
63  =>  0.058368074009046554
64  =>  0.05379201700673732
65  =>  0.04957472287340911
66  =>  0.045688064600133846
67  =>  0.04210612033548335
68  =>  0.038805000501181464
69  =>  0.035762688461888834
70  =>  0.03295889368647675
71  =>  0.030374916421456986
72  =>  0.02799352297401475
73  =>  0.025798830772851992
74  =>  0.0237762024402604
75  =>  0.02191214816894399
76  =>  0.020194235752498776
77  =>  0.018611007669502876
78  =>  0.017151904668213848
79  =>  0.015807195342225883
80  =>  0.014567911227395378
81  =>  0.013425786987167579
82  =>  0.012373205287373639
83  =>  0.011403145992843543
84  =>  0.010509139347004611
85  =>  0.00968522282219945
86  =>  0.008925901352939013
87  =>  0.008226110686868595
88  =>  0.007581183609018099
89  =>  0.006986818814071082
90  =>  0.006439052219047909
91  =>  0.005934230525074554
92  =>  0.005468986851908708
93  =>  0.005040218282719065
94  =>  0.004645065169353892
95  =>  0.004280892060076547
96  =>  0.003945270122566546
97  =>  0.003635960944957329
98  =>  0.003350901606872675
99  =>  0.003088190920893857
x= 0.01687031935884968 y= 0.050610958076549
```
* autograd0.py
```
import torch 
'''
x = net.variable(1)
y = net.variable(3)
x2 = net.mul(x, x)
y2 = net.mul(y, y)
o  = net.add(x2, y2)

print('net.forward()=', net.forward())
print('net.backwward()')
net.backward() 
'''
#上面為註解
x = torch.tensor(1., requires_grad=True)
y = torch.tensor(3., requires_grad=True)
x2 = x*x
y2 = y*y
f = x2+y2

f.backward()

print(x.grad)    # x.grad = 2 
print(y.grad)    # y.grad = 6
print(f.item())  # f.value = 10
```
* autograd1.py
    * 浮點數才能算梯度
```
import torch
x = torch.tensor(1.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)
f = x*x+y*y
f.backward()     # automatically calculates the gradient
print(x.grad)    # ∂f/∂x = 2 X梯度為2
print(y.grad)    # ∂f/∂y = 6 Y梯度為6
print(f.item())  # z值
```
* autograd2.py
```
import torch
x = torch.tensor([1.0,3.0], requires_grad=True)
n = x.norm()
f = n*n
f.backward()
print('f=', f.item())
print('x.grad=', x.grad)
```
* torchGd1.py
    * 只能有單一參數
```
import torch
import math

dtype = torch.float
x = torch.randn((), dtype=dtype, requires_grad=True) # torch.linspace(-math.pi, math.pi, 2000)

def loss_fn(x):
	loss = x*x-4*x+4
	return loss

def GD(x, loss_fn, loop_max = 10000, learning_rate = 1e-3):
	for t in range(loop_max):
		loss = loss_fn(x)
		if t % 100 == 99:
			print(t, 'x=', x.item(), 'loss=', loss.item())
		loss.backward()
		with torch.no_grad():
			x -= learning_rate * x.grad
			x.grad = None

GD(x, loss_fn, loop_max = 5000)

print(f'Result: x = {x.item()} loss={loss_fn(x)}')
```
* torchGd2.py
    * 多參數
```
import torch
import math

dtype = torch.float
x = torch.randn((), dtype=dtype, requires_grad=True) # torch.linspace(-math.pi, math.pi, 2000)

def loss_fn(parameters):
	x = parameters[0]
	loss = x*x-4*x+4
	return loss

def GD(parameters, loss_fn, loop_max = 10000, learning_rate = 1e-3):
	for t in range(loop_max):
		loss = loss_fn(parameters)
		if t % 100 == 99:
			print(t, 'parameters=', parameters, 'loss=', loss.item())
		loss.backward()
		with torch.no_grad():
			for x in parameters:
				x -= learning_rate * x.grad
				x.grad = None

params = [x]
GD(params, loss_fn, loop_max = 5000)

print(f'Result: parameters = {params} loss={loss_fn(params)}')
```
* torchGd3.py
    * 直接找到最小值
```
import torch
x = torch.tensor([1.0,2.0], requires_grad=True)

step = 0.01
for i in range(500):
    z = x.norm()
    z.backward()
    # print('z=', z)
    # print('x.grad=', x.grad)
    with torch.no_grad():
        x -= step * x.grad
        x.grad.zero_()
    # print('x=', x)
print('x=', x)
```
* regression1train.py
```
import torch
import math

# Create Tensors to hold input and outputs.
x = torch.linspace(-math.pi, math.pi, 2000)
y = 0+1*x+2*x**2+3*x**3 # torch.sin(x)

# Prepare the input tensor (x, x^2, x^3).
p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)
'''
xx 為 tensor (x, x^2, x^3) 的陣列，如下
PS D:\pmedia> python
Python 3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> import math
>>> x = torch.linspace(-math.pi, math.pi, 2000)
>>> x
tensor([-3.1416, -3.1384, -3.1353,  ...,  3.1353,  3.1384,  3.1416])
>>> p = torch.tensor([1, 2, 3])
>>> p
tensor([1, 2, 3])
>>> x.unsqueeze(-1)            
tensor([[-3.1416],
        [-3.1384],
        [-3.1353],
        ...,
        [ 3.1353],
        [ 3.1384],
        [ 3.1416]])
>>> x.unsqueeze(-1).pow(p)
tensor([[ -3.1416,   9.8696, -31.0063],
        [ -3.1384,   9.8499, -30.9133],
        [ -3.1353,   9.8301, -30.8205],
        ...,
        [  3.1353,   9.8301,  30.8205],
        [  3.1384,   9.8499,  30.9133],
        [  3.1416,   9.8696,  31.0063]])
'''
# Use the nn package to define our model and loss function.
model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),
    torch.nn.Flatten(0, 1)
)

def loss_fn(output, target):
    loss = torch.mean((output - target)**2)
    return loss
# loss_fn = torch.nn.MSELoss(reduction='sum')

def GD(inputs, outputs, model, loss_fn, loop_max = 10000, learning_rate = 1e-3):
	# Use the optim package to define an Optimizer that will update the weights of
	# the model for us. Here we will use RMSprop; the optim package contains many other
	# optimization algorithms. The first argument to the RMSprop constructor tells the
	# optimizer which Tensors it should update.
	# 註 參考 hinton https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
	# rmsprop: Divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.
	optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
	for t in range(loop_max):
		# Forward pass: compute predicted y by passing x to the model.
		predicats = model(inputs)

		# Compute and print loss.
		loss = loss_fn(predicats, outputs)
		if t % 100 == 99:
			print(t, loss.item())

		# Before the backward pass, use the optimizer object to zero all of the
		# gradients for the variables it will update (which are the learnable
		# weights of the model). This is because by default, gradients are
		# accumulated in buffers( i.e, not overwritten) whenever .backward()
		# is called. Checkout docs of torch.autograd.backward for more details.
		optimizer.zero_grad()

		# Backward pass: compute gradient of the loss with respect to model
		# parameters
		loss.backward()

		# Calling the step function on an Optimizer makes an update to its
		# parameters
		optimizer.step()

GD(inputs=xx, outputs=y, model=model, loss_fn=loss_fn)
linear_layer = model[0]
# 為何以下這行用 linear_layer.weight[:, 0].item() ? 這是甚麼意思?
# 猜測:因為 .weight 是個 torch 物件，而非 list，透過 weight[:] 先轉為 list 之後，再取出第 i 個...
print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')
torch.save(model, 'model.ckpt')
```
* regression2predict.py
```
import torch
import math

model = torch.load('model.ckpt')
inputs = torch.tensor([[2, 4, 8]], dtype=torch.float)
predict = model(inputs)
print(f'predict={predict}')
```
* torchRegression4nn.py
```
import torch
import math


# Create Tensors to hold input and outputs.
x = torch.linspace(-math.pi, math.pi, 2000)
y = 0+1*x+2*x**2+3*x**3 # torch.sin(x)

# For this example, the output y is a linear function of (x, x^2, x^3), so
# we can consider it as a linear layer neural network. Let's prepare the
# tensor (x, x^2, x^3).
p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)
'''
xx 為 tensor (x, x^2, x^3) 的陣列，如下
>>> x.unsqueeze(-1)
tensor([[-3.1416],
        [-3.1384],
        [-3.1353],
        ...,
        [ 3.1353],
        [ 3.1384],
        [ 3.1416]])
>>> x.unsqueeze(-1).pow(p)
tensor([[ -3.1416,   9.8696, -31.0063],
        [ -3.1384,   9.8499, -30.9133],
        [ -3.1353,   9.8301, -30.8205],
        ...,
        [  3.1353,   9.8301,  30.8205],
        [  3.1384,   9.8499,  30.9133],
        [  3.1416,   9.8696,  31.0063]])
'''

# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape
# (3,), for this case, broadcasting semantics will apply to obtain a tensor
# of shape (2000, 3) 

# Use the nn package to define our model as a sequence of layers. nn.Sequential
# is a Module which contains other Modules, and applies them in sequence to
# produce its output. The Linear Module computes output from input using a
# linear function, and holds internal Tensors for its weight and bias.
# The Flatten layer flatens the output of the linear layer to a 1D tensor,
# to match the shape of `y`.
model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),
    torch.nn.Flatten(0, 1) # 將整個層攤平 -- torch.nn.Flatten(start_dim=1, end_dim=-1)
)

# The nn package also contains definitions of popular loss functions; in this
# case we will use Mean Squared Error (MSE) as our loss function.
loss_fn = torch.nn.MSELoss(reduction='sum')

learning_rate = 1e-6
for t in range(2000):

    # Forward pass: compute predicted y by passing x to the model. Module objects
    # override the __call__ operator so you can call them like functions. When
    # doing so you pass a Tensor of input data to the Module and it produces
    # a Tensor of output data.
    y_pred = model(xx)

    # Compute and print loss. We pass Tensors containing the predicted and true
    # values of y, and the loss function returns a Tensor containing the
    # loss.
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # Zero the gradients before running the backward pass.
    model.zero_grad()

    # Backward pass: compute gradient of the loss with respect to all the learnable
    # parameters of the model. Internally, the parameters of each Module are stored
    # in Tensors with requires_grad=True, so this call will compute gradients for
    # all learnable parameters in the model.
    loss.backward()

    # Update the weights using gradient descent. Each parameter is a Tensor, so
    # we can access its gradients like we did before.
    with torch.no_grad(): # 這裡用 with 的原因是完成後會釋放資源。請參考 https://blog.gtwang.org/programming/python-with-context-manager-tutorial/
        for param in model.parameters():
            param -= learning_rate * param.grad

# You can access the first layer of `model` like accessing the first item of a list
linear_layer = model[0]

# For linear layer, its parameters are stored as `weight` and `bias`.
print(f'linear_layer.weight={linear_layer.weight}') 
print(f'linear_layer.weight[:]={linear_layer.weight[:]}') 
print(f'linear_layer.weight[:,0]={linear_layer.weight[:,0]}') 
print(f'linear_layer.weight[:,1]={linear_layer.weight[:,1]}') 
# 為何以下這行用 linear_layer.weight[:, 0].item() ? 這是甚麼意思?
# 猜測:因為 .weight 是個 torch 物件，而非 list，透過 weight[:] 先轉為 list 之後，再取出第 i 個...
print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')
```
* torchRegression5optim
```
import torch
import math


# Create Tensors to hold input and outputs.
x = torch.linspace(-math.pi, math.pi, 2000)
y = 0+1*x+2*x**2+3*x**3 # torch.sin(x)

# Prepare the input tensor (x, x^2, x^3).
p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)

# Use the nn package to define our model and loss function.
model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),
    torch.nn.Flatten(0, 1)
)
loss_fn = torch.nn.MSELoss(reduction='sum')

# Use the optim package to define an Optimizer that will update the weights of
# the model for us. Here we will use RMSprop; the optim package contains many other
# optimization algorithms. The first argument to the RMSprop constructor tells the
# optimizer which Tensors it should update.
learning_rate = 1e-3
optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
for t in range(10000):
    # Forward pass: compute predicted y by passing x to the model.
    y_pred = model(xx)

    # Compute and print loss.
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # Before the backward pass, use the optimizer object to zero all of the
    # gradients for the variables it will update (which are the learnable
    # weights of the model). This is because by default, gradients are
    # accumulated in buffers( i.e, not overwritten) whenever .backward()
    # is called. Checkout docs of torch.autograd.backward for more details.
    optimizer.zero_grad()

    # Backward pass: compute gradient of the loss with respect to model
    # parameters
    loss.backward()

    # Calling the step function on an Optimizer makes an update to its
    # parameters
    optimizer.step()


linear_layer = model[0]
print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')
```
* torchRegression6custom
```
import torch
import math


class Polynomial3(torch.nn.Module):
    def __init__(self):
        """
        In the constructor we instantiate four parameters and assign them as
        member parameters.
        """
        super().__init__()
        self.a = torch.nn.Parameter(torch.randn(()))
        self.b = torch.nn.Parameter(torch.randn(()))
        self.c = torch.nn.Parameter(torch.randn(()))
        self.d = torch.nn.Parameter(torch.randn(()))

    def forward(self, x):
        """
        In the forward function we accept a Tensor of input data and we must return
        a Tensor of output data. We can use Modules defined in the constructor as
        well as arbitrary operators on Tensors.
        """
        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3

    def string(self):
        """
        Just like any class in Python, you can also define custom method on PyTorch modules
        """
        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'


# Create Tensors to hold input and outputs.
x = torch.linspace(-math.pi, math.pi, 2000)
y = 0+1*x+2*x**2+3*x**3 # torch.sin(x)

# Construct our model by instantiating the class defined above
model = Polynomial3()

# Construct our loss function and an Optimizer. The call to model.parameters()
# in the SGD constructor will contain the learnable parameters (defined 
# with torch.nn.Parameter) which are members of the model.
criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)
for t in range(2000):
    # Forward pass: Compute predicted y by passing x to the model
    y_pred = model(x)

    # Compute and print loss
    loss = criterion(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(f'Result: {model.string()}')
```
* torchRegression7cws
```
import random
import torch
import math


class DynamicNet(torch.nn.Module):
    def __init__(self):
        """
        In the constructor we instantiate five parameters and assign them as members.
        """
        super().__init__()
        self.a = torch.nn.Parameter(torch.randn(()))
        self.b = torch.nn.Parameter(torch.randn(()))
        self.c = torch.nn.Parameter(torch.randn(()))
        self.d = torch.nn.Parameter(torch.randn(()))
        self.e = torch.nn.Parameter(torch.randn(()))

    def forward(self, x):
        """
        For the forward pass of the model, we randomly choose either 4, 5
        and reuse the e parameter to compute the contribution of these orders.

        Since each forward pass builds a dynamic computation graph, we can use normal
        Python control-flow operators like loops or conditional statements when
        defining the forward pass of the model.

        Here we also see that it is perfectly safe to reuse the same parameter many
        times when defining a computational graph.
        """
        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3
        for exp in range(4, random.randint(4, 6)):
            y = y + self.e * x ** exp
        return y

    def string(self):
        """
        Just like any class in Python, you can also define custom method on PyTorch modules
        """
        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'


# Create Tensors to hold input and outputs.
x = torch.linspace(-math.pi, math.pi, 2000)
y = 0+1*x+2*x**2+3*x**3 # torch.sin(x)

# Construct our model by instantiating the class defined above
model = DynamicNet()

# Construct our loss function and an Optimizer. Training this strange model with
# vanilla stochastic gradient descent is tough, so we use momentum
criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)
for t in range(30000):
    # Forward pass: Compute predicted y by passing x to the model
    y_pred = model(x)

    # Compute and print loss
    loss = criterion(y_pred, y)
    if t % 2000 == 1999:
        print(t, loss.item())

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(f'Result: {model.string()}')
```